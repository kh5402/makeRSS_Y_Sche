import os
import re
from bs4 import BeautifulSoup
from pyppeteer import launch
from xml.etree.ElementTree import Element, SubElement, tostring, ElementTree
from datetime import datetime, timedelta
import xml.dom.minidom
import xml.etree.ElementTree as ET
import asyncio
import requests
from html import unescape as html_unescape
from urllib.parse import urlparse, parse_qs

# æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æƒ…å ±å–å¾—
def get_existing_schedules(file_name):
    existing_schedules = set()
    try:
        tree = ET.parse(file_name)
        root = tree.getroot()
        for item in root.findall(".//item"):
            date = item.find('pubDate').text
            title = html_unescape(item.find('title').text)
            url = html_unescape(item.find('link').text)
            category = item.find('category').text
            start_time = item.find('start_time').text
            existing_schedules.add((date, title, url, category, start_time))
    except FileNotFoundError:
        print(f"File not found: {file_name}")
    except ET.ParseError:
        print(f"Error parsing XML file: {file_name}")
    return existing_schedules

# URLãŒå¯å¤‰ã™ã‚‹éƒ¨åˆ†ã‚’é™¤å¤–ã—ã¦URLã‚’ç¢ºèªã™ã‚‹
def extract_url_part(url):
    parsed_url = urlparse(url)
    path = parsed_url.path.split("/")[-1]  # /103002 ã‚„ /102232 ã‚’å–å¾—
    query = parse_qs(parsed_url.query)
    unique_part = f"{path}_{query.get('pri1', [''])[0]}_{query.get('wd00', [''])[0]}_{query.get('wd01', [''])[0]}_{query.get('wd02', [''])[0]}"
    return unique_part

async def main():
    # Discordã®webhook URLã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    # webhook_url = os.environ['WEBHOOK_URL']

    # æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã€ãã®æƒ…å ±ã‚’å–å¾—
    existing_file = 'Y_Sche.xml'
    existing_schedules = get_existing_schedules(existing_file)

    # å¾Œã§é‡è¤‡ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ãã®ç‚ºã®ä¸€è¦§
    existing_schedules_check = {(date, extract_url_part(url)) for date, _, url, _, _ in existing_schedules}

    # æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
    new_schedules = []

    # å…ˆæœˆã®1æ—¥ã‹ã‚‰3ãƒ¶æœˆå…ˆã¾ã§ã®yyyymmã‚’ç”Ÿæˆ
    start_date = (datetime.today().replace(day=1) - timedelta(days=1)).replace(day=1)
    end_date = start_date + timedelta(days=90)
    current_date = start_date

    try:
        # Pyppeteerã§ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã
        browser = await launch(
            executablePath='/usr/bin/chromium-browser',
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--disable-gpu'
            ],
            defaultViewport=None,
            userDataDir='./user_data',
            logLevel='INFO'  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ä¸Šã’ã‚‹
        )
        print(f"Chromium launched successfully: {browser}")

        while current_date <= end_date:
            yyyymm = current_date.strftime('%Y%m')
            url = f"https://www.nogizaka46.com/s/n46/media/list?dy={yyyymm}&members={{%22member%22:[%2255387%22]}}"
            print(f"Fetching URL: {url}")

            page = await browser.newPage()
            print(f"Page created: {page}")

            try:
                # 1. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã®å»¶é•·: (å¿…è¦ã«å¿œã˜ã¦ timeout å€¤ã‚’èª¿æ•´)
                response = await page.goto(url, timeout=60000)
                print(f"Navigated to URL: {url}, Status: {response.status}")

                # 1. ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®æ§‹é€ å¤‰æ›´ã®ç¢ºèª:
                print(f"HTML Content: {await page.content()}")  # HTML ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡ºåŠ›

                # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
                await page.waitForNavigation()

                # ***** ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ã‚’å«ã‚€è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ *****
                await page.waitForSelector('.sc--day')  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ã‚’å«ã‚€è¦ç´ ã®ã‚»ãƒ¬ã‚¯ã‚¿

                # ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
                html = await page.content()

                # BeautifulSoupã§è§£æ
                soup = BeautifulSoup(html, 'html.parser')

                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ã®å–å¾—
                day_schedules = soup.find_all('div', class_='sc--day')
                print(f"day_schedules: {day_schedules}")

                # å„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æƒ…å ±ã‚’å–å¾—
                for day_schedule in day_schedules:
                    date_tag = day_schedule.find('div', class_='sc--day__hd js-pos a--tx')
                    if date_tag is None:
                        continue
                    date = f"{yyyymm[:4]}/{yyyymm[4:]}/{date_tag.find('p', class_='sc--day__d f--head').text}"

                    schedule_links = day_schedule.find_all('a', class_='m--scone__a hv--op')

                    for link in schedule_links:
                        title = re.search(r'<p class="m--scone__ttl">(.*?)</p>', str(link.find('p', class_='m--scone__ttl')))
                        if title:
                            title = title.group(1)
                        else:
                            title = ""
                        title_tag = link.find('p', class_='m--scone__ttl')
                        if title_tag:
                            title = title_tag.get_text()
                        title = html_unescape(str(title))

                        url = link['href']
                        url = html_unescape(str(url))

                        category = link.find('p', class_='m--scone__cat__name').text
                        start_time_tag = link.find('p', class_='m--scone__start')
                        start_time = start_time_tag.text if start_time_tag else ''

                        # æ–°è¦æƒ…å ±ã®ç¢ºèª URLã¯å¤‰ã‚ã‚‹ã®ã§æ—¥ä»˜ã¨ã‚¿ã‚¤ãƒˆãƒ«ã ã‘ã§ç¢ºèª
                        extracted_url = extract_url_part(url)
                        try:
                            datetime.strptime(date, "%Y/%m/%d")  # ã“ã“ã§æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ãƒã‚§ãƒƒã‚¯
                            if (date, extracted_url) not in existing_schedules_check:
                                new_schedules.append((date, title, url, category, start_time))
                                print(f"æ–°è¦æƒ…å ±ã‚’è¿½åŠ : {date, title, url, category, start_time}")  # ã“ã“ã§æ–°è¦æƒ…å ±ã‚’å‡ºåŠ›
                        except ValueError:
                            print(f"æ–°è¦æƒ…å ±ã®æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒãŠã‹ã—ã„ã‹ã‚‰ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã§ï¼æ—¥ä»˜: {date}")

            except asyncio.TimeoutError:
                print(f"Navigation Timeout Exceeded for URL: {url}")
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†ã‚’ã“ã“ã«è¨˜è¿°

            # æ¬¡ã®æœˆã¸
            current_date = (current_date + timedelta(days=31)).replace(day=1)
            if current_date.day != 1:  # æœˆã®æœ€åˆã®æ—¥ã§ã¯ãªã„å ´åˆ
                current_date = (current_date + timedelta(days=1)).replace(day=1)  # æœˆã‚’1ã¤é€²ã‚ã‚‹

    except Exception as e:
        print(f"Error occurred during browser operation: {e}")

    finally:
        if browser:
            await browser.close()
            print("Chromium closed.")

    # æ–°è¦æƒ…å ±ãŒã‚ã‚Œã°ã€Discordã¸é€šçŸ¥
    print('# æ–°è¦æƒ…å ±ãŒã‚ã‚Œã°ã€Discordã¸é€šçŸ¥')
    print(new_schedules)
    # for date, title, url, category, start_time in new_schedules:
    #    discord_message = f"æ–°ã—ã„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚„ã§ï¼ğŸ‰ğŸ’–\næ—¥ä»˜: {date}\né–‹å§‹æ™‚é–“: {start_time}\nã‚«ãƒ†ã‚´ãƒª: {category}\nã‚¿ã‚¤ãƒˆãƒ«: {title}\nURL: {url}\n"
    #    payload = {"content": discord_message}
    #    await asyncio.sleep(1)

    # Discordã¸ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
    # response = requests.post(webhook_url, json=payload)
    # if response.status_code != 204:
    #    print(f"é€šçŸ¥ã«å¤±æ•—ã—ãŸã§: {response.text}") # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º

    # æ—¢å­˜ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ã‚‚ãƒªã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
    existing_schedules_list = [(date, title, url, category, start_time) for date, title, url, category, start_time in
                                existing_schedules]

    # æ—¢å­˜ã®æƒ…å ±ã¨æ–°è¦æƒ…å ±ã‚’åˆã‚ã›ã‚‹
    all_schedules = existing_schedules_list + new_schedules

    # æ—¥ä»˜ã®é™é †ã«ã‚½ãƒ¼ãƒˆ
    all_schedules.sort(key=lambda x: datetime.strptime(x[0], "%Y/%m/%d"), reverse=True)

    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
    rss = Element("rss", version="2.0")
    channel = SubElement(rss, "channel")
    SubElement(channel, "title").text = "å¼“æœ¨å¥ˆæ–¼ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"
    SubElement(channel, "description").text = ""
    SubElement(channel, "link").text = ""
    for date, title, url, category, start_time in all_schedules:
        item = SubElement(channel, "item")
        SubElement(item, "title").text = title
        SubElement(item, "link").text = url
        SubElement(item, "pubDate").text = date
        SubElement(item, "category").text = category
        SubElement(item, "start_time").text = start_time

    xml_str = xml.dom.minidom.parseString(tostring(rss)).toprettyxml(indent="   ")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(existing_file, 'w', encoding='utf-8') as f:
        f.write(xml_str)


# éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œ
if __name__ == "__main__":
    asyncio.get_event_loop().run_until_complete(main())
